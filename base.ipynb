{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26f2ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple, Iterable\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass\n",
    "import argparse\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd3fb3",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86253939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent_message_sequential_latent_mas(role: str, question: str, context: str = \"\", method=None, args=None):\n",
    "\n",
    "    system_message = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "\n",
    "    assert method in [\"latent_mas\"], \"this prompt only for latent_mas method\"\n",
    "    assert \"qwen\" in args.model_name.lower(), \"this prompt only for qwen models\"\n",
    "\n",
    "    if role == \"planner\":\n",
    "        user_prompt = f\"\"\"You are a Planner Agent. Given an input question, design a clear, step-by-step plan for how to solve the question.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Your outlined plan should be concise with a few bulletpoints for each step. Do not produce the final answer.\n",
    "Now output your plan to solve the question below:\n",
    "\"\"\"\n",
    "    \n",
    "    elif role == \"critic\":\n",
    "        user_prompt = f\"\"\"\n",
    "Question: {question}\n",
    "\n",
    "You are a Critic Agent to evaluate the correctness of the input plan for the given question and provide helpful feedback for improving the plan.\n",
    "The plan information is provided in latent KV representation format. Review the plan and question and output:\n",
    "(1) original plan contents\n",
    "(2) constructive feedback on the original plan.\n",
    "\n",
    "Format your response as follows:\n",
    "Original Plan: [Copy the provided Planner Agent's plan here]\n",
    "Feedback: [Your detailed feedback to improve the plan here]\n",
    "\n",
    "Now, output your response below:\n",
    "\"\"\"\n",
    "    \n",
    "    elif role == \"refiner\":\n",
    "        user_prompt = f\"\"\"\n",
    "Question: {question}\n",
    "\n",
    "You are a Refiner Agent to provide a refined step-by-step plan for solving the given question.\n",
    "You are provided with:\n",
    "(1) latent-format information: a previous plan with feedback\n",
    "(2) text-format information: the input question you need to solve.\n",
    "\n",
    "Based on the input, write a refined and improved plan to solve the question. Make sure your output plan is correct and concise.\n",
    "\n",
    "Now, output your refined plan below:\n",
    "\"\"\"\n",
    "    \n",
    "    elif role == \"judger\":\n",
    "        if args.task in ['gsm8k', 'aime2024', 'aime2025']:\n",
    "            user_prompt = f\"\"\"\n",
    "Target Question: {question}\n",
    "\n",
    "You are a helpful assistant. You are provided with latent information for reference and a target question to solve. \n",
    "\n",
    "The latent information might contain irrelevant contents. Ignore it if it is not helpful for solving the target question.\n",
    "\n",
    "You must reason step-by-step to solve the provided Target Question without outputting other irrelevant information.\n",
    "\n",
    "Now, reason step by step and output the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\"\"\"\n",
    "        \n",
    "        elif args.task in [\"arc_easy\", \"arc_challenge\", \"gpqa\", 'medqa']:\n",
    "            user_prompt = f\"\"\"\n",
    "Target Question: {question}\n",
    "\n",
    "You are a helpful assistant. You are provided with latent information for reference and a target question to solve. \n",
    "\n",
    "The latent information might contain irrelevant contents. Ignore it if it is not helpful for solving the target question.\n",
    "\n",
    "You must reason step-by-step to solve the provided Target Question without outputting other irrelevant information.\n",
    "Your final answer must be selected from A,B,C,D. For example \\\\boxed{{A}}. Do not add any other contents inside the box.\n",
    "\n",
    "Now, reason step by step and output the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\"\"\"\n",
    "\n",
    "        elif args.task in [\"mbppplus\", \"humanevalplus\"]:\n",
    "            user_prompt = f\"\"\"\n",
    "Target Question: {question}\n",
    "\n",
    "You are a helpful assistant. You are provided with latent information for reference and a target question to solve.\n",
    "\n",
    "The latent information might contain irrelevant contents. Ignore it if it is not helpful for solving the target question.\n",
    "\n",
    "You must reason step-by-step to solve the provided Target Question without outputting other irrelevant information.\n",
    "You must put all python code as self-contained Python function in markdown code blocks. For example ```python\n",
    "import math\n",
    "def add(a, b):\n",
    "    return a + b```. Do not add any other contents inside the markdown code block.\n",
    "\n",
    "Now, reason step by step and output the final answer inside ```python\n",
    "YOUR_PYTHON_CODE\n",
    "```.\n",
    "\"\"\"\n",
    "\n",
    "        elif args.task in [\"winogrande\"]:\n",
    "            user_prompt = f\"\"\"\n",
    "Target Question: {question}\n",
    "\n",
    "You are a helpful assistant. You are provided with latent information for reference and a target question to solve. \n",
    "\n",
    "The latent information might contain irrelevant contents. Ignore it if it is not helpful for solving the target question.\n",
    "\n",
    "You must reason step-by-step to solve the provided Target Question without outputting other irrelevant information.\n",
    "Your final answer must be selected from 1 and 2. For example \\\\boxed{{1}} or \\\\boxed{{2}}. Do not add any other contents inside the box.\n",
    "\n",
    "Now, reason step by step and output the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\"\"\"\n",
    "\n",
    "        else: \n",
    "            raise NotImplementedError(f\"Task {args.task} not implemented in v5 judger prompt.\")\n",
    "        \n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_agent_message_hierarchical_latent_mas(role: str, question: str, context: str = \"\", method=None, args=None):\n",
    "\n",
    "    system_message = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "\n",
    "    assert method in [\"latent_mas\"], \"this prompt only for latent_mas method\"\n",
    "    assert \"qwen\" in args.model_name.lower(), \"this prompt only for qwen models\"\n",
    "\n",
    "    if args.task in ['gsm8k', 'aime2024', 'aime2025']:\n",
    "        if role == \"planner\":\n",
    "            user_content = f\"\"\"\n",
    "You are a math agent. Given the input question, reason step-by-step and put the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "    \n",
    "        elif role == \"critic\":\n",
    "            user_content = f\"\"\"\n",
    "You are a science agent. Given the input question, reason step-by-step and put the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\n",
    "Input Question: {question}     \n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "    \n",
    "        elif role == \"refiner\":\n",
    "            user_content = f\"\"\"\n",
    "You are a code agent. Given the input question, reason step-by-step and put the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:       \n",
    "\"\"\"\n",
    "        elif role == \"judger\":\n",
    "            user_content = f\"\"\"\n",
    "You are a task summarizer. Given the input question and responses from previous agents as reference, reason step-by-step and put the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "\n",
    "    elif args.task in [\"arc_easy\", \"arc_challenge\", \"gpqa\", \"medqa\"]:\n",
    "\n",
    "        if args.task == \"medqa\":\n",
    "\n",
    "            if role == \"planner\":\n",
    "                user_content = f\"\"\"\n",
    "You are a math agent. Given the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "Your final answer must be selected from A,B,C,D. \n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "            elif role == \"critic\":\n",
    "                user_content = f\"\"\"\n",
    "You are a science agent. Given the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "Your final answer must be selected from A,B,C,D. \n",
    "\n",
    "Input Question: {question}     \n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "            elif role == \"refiner\":\n",
    "                user_content = f\"\"\"\n",
    "You are a code agent. Given the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "Your final answer must be selected from A,B,C,D. \n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:       \n",
    "\"\"\"\n",
    "            elif role == \"judger\":\n",
    "\n",
    "                user_content = f\"\"\"\n",
    "You are a task summarizer. Given the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "Your final answer must be selected from A,B,C,D. \n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "\n",
    "        else:\n",
    "            if role == \"planner\":\n",
    "                user_content = f\"\"\"\n",
    "You are a math agent. Given the input question, reason step-by-step and put the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "Your final answer must be selected from A,B,C,D. For example \\\\boxed{{A}}. Do not add any other contents inside the box.\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "    \n",
    "            elif role == \"critic\":\n",
    "                user_content = f\"\"\"\n",
    "You are a science agent. Given the input question, reason step-by-step and put the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "Your final answer must be selected from A,B,C,D. For example \\\\boxed{{A}}. Do not add any other contents inside the box.\n",
    "\n",
    "Input Question: {question}     \n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "    \n",
    "            elif role == \"refiner\":\n",
    "                user_content = f\"\"\"\n",
    "You are a code agent. Given the input question, reason step-by-step and put the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "Your final answer must be selected from A,B,C,D. For example \\\\boxed{{A}}. Do not add any other contents inside the box.\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:       \n",
    "\"\"\"\n",
    "            elif role == \"judger\":\n",
    "\n",
    "                user_content = f\"\"\"\n",
    "You are a task summarizer. Given the input question and responses from previous agents as reference, reason step-by-step and put the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "Your final answer must be selected from A,B,C,D. For example \\\\boxed{{A}}. Do not add any other contents inside the box.\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "\n",
    "    elif args.task in [\"mbppplus\", \"humanevalplus\"]:\n",
    "        \n",
    "        if role == \"planner\":\n",
    "            user_content = f\"\"\"\n",
    "You are a math agent. Given the input question, reason step by step: please provide an efficient and self-contained Python function that solves the following problem in a markdown code block:\\n```\\nYOUR_PYTHON_CODE\\n```.\n",
    "You must put all python code as self-contained Python function in markdown code blocks. For example ```python\n",
    "import math\n",
    "def add(a, b):\n",
    "    return a + b```. Do not add any other contents inside the markdown code block. \n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "        elif role == \"critic\":\n",
    "            user_content = f\"\"\"\n",
    "You are a science agent. Given the input question, reason step by step: please provide an efficient and self-contained Python function that solves the following problem in a markdown code block:\\n```\\nYOUR_PYTHON_CODE\\n```.\n",
    "You must put all python code as self-contained Python function in markdown code blocks. For example ```python\n",
    "import math\n",
    "def add(a, b):\n",
    "    return a + b```. Do not add any other contents inside the markdown code block. \n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "        elif role == \"refiner\":\n",
    "            user_content = f\"\"\"\n",
    "You are a code agent. Given the input question, reason step by step: please provide an efficient and self-contained Python function that solves the following problem in a markdown code block:\\n```\\nYOUR_PYTHON_CODE\\n```.\n",
    "You must put all python code as self-contained Python function in markdown code blocks. For example ```python\n",
    "import math\n",
    "def add(a, b):\n",
    "    return a + b```. Do not add any other contents inside the markdown code block. \n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:       \n",
    "\"\"\"\n",
    "        elif role == \"judger\":\n",
    "            user_content = f\"\"\"\n",
    "You are a task summarizer. Given the input question and responses from previous agents as reference, reason step by step: please provide an efficient and self-contained Python function that solves the following problem in a markdown code block:\\n```\\nYOUR_PYTHON_CODE\\n```.\n",
    "You must put all python code as self-contained Python function in markdown code blocks. For example ```python\n",
    "import needed_library\n",
    "def FUNC_NAME(a, b):\n",
    "    return a + b```. Do not add any other contents inside the markdown code block. \n",
    "    \n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "\n",
    "    elif args.task in [\"winogrande\"]:\n",
    "        if role == \"planner\":\n",
    "            user_content = f\"\"\"\n",
    "You are a math agent. Given the input question, reason step-by-step and put the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\"Your final answer must be selected from 1 and 2. For example \\\\boxed{{1}} or \\\\boxed{{2}}. Do not add any other contents inside the box.\"\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "    \n",
    "        elif role == \"critic\":\n",
    "            user_content = f\"\"\"\n",
    "You are a science agent. Given the input question, reason step-by-step and put the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\"Your final answer must be selected from 1 and 2. For example \\\\boxed{{1}} or \\\\boxed{{2}}. Do not add any other contents inside the box.\"\n",
    "\n",
    "Input Question: {question}     \n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "    \n",
    "        elif role == \"refiner\":\n",
    "            user_content = f\"\"\"\n",
    "You are a code agent. Given the input question, reason step-by-step and put the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\"Your final answer must be selected from 1 and 2. For example \\\\boxed{{1}} or \\\\boxed{{2}}. Do not add any other contents inside the box.\"\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:       \n",
    "\"\"\"\n",
    "        elif role == \"judger\":\n",
    "            user_content = f\"\"\"\n",
    "You are a task summarizer. Given the input question and responses from previous agents as reference, reason step-by-step and put the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\"Your final answer must be selected from 1 and 2. For example \\\\boxed{{1}} or \\\\boxed{{2}}. Do not add any other contents inside the box.\"\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_agent_messages_sequential_text_mas(role: str, question: str, context: str = \"\", method=None, args=None):\n",
    "\n",
    "    system_message = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "\n",
    "    assert method in [\"text_mas\"], \"only for text_mas method\"\n",
    "    assert \"qwen\" in args.model_name.lower(), \"only for qwen models\"\n",
    "\n",
    "    # truncate context if needed\n",
    "    ctx = context[: args.text_mas_context_length]\n",
    "\n",
    "    if role == \"planner\":\n",
    "        user_content = f\"\"\"\n",
    "You are a Planner Agent. Given an input question, design a clear, step-by-step plan for how to solve the question.\n",
    "\n",
    "## Input Question:\n",
    "{question}\n",
    "\n",
    "Your outlined plan should be concise with a few bullet points for each step. Do not produce the final answer.\n",
    "\n",
    "## Format your response as follows:\n",
    "Planner Agent's Output:\n",
    "[Your detailed plan here]\n",
    "\n",
    "Now output your plan to solve the question below:\n",
    "\"\"\"\n",
    "\n",
    "    elif role == \"critic\":\n",
    "        user_content = f\"\"\"\n",
    "You are a Critic Agent. You are provided with:\n",
    "(1) the original question, and\n",
    "(2) the Planner Agent's plan in text format.\n",
    "\n",
    "Your job is to carefully evaluate the correctness and completeness of the plan and provide helpful feedback.\n",
    "\n",
    "## Input Question:\n",
    "{question}\n",
    "\n",
    "## Plan from Planner Agent:\n",
    "{ctx}\n",
    "\n",
    "## Format your response as follows:\n",
    "Critic Agent's Output:\n",
    "Original Plan: [Copy the provided Planner Agent's plan here]\n",
    "Feedback: [Your detailed feedback to improve the plan here]\n",
    "\n",
    "Now, output your response below:\n",
    "\"\"\"\n",
    "\n",
    "    elif role == \"refiner\":\n",
    "        user_content = f\"\"\"\n",
    "You are a Refiner Agent. You are provided with:\n",
    "(1) the original question, and\n",
    "(2) the Planner Agent's plan together with Critic Agent's feedback in text format.\n",
    "\n",
    "Your job is to incorporate the feedback and produce an improved, refined step-by-step plan.\n",
    "\n",
    "## Input Question:\n",
    "{question}\n",
    "\n",
    "## Original Plan and Critic Feedback:\n",
    "{ctx}\n",
    "\n",
    "## Format your response as follows:\n",
    "Refiner Agent's Output:\n",
    "[Your refined and improved plan here]\n",
    "\n",
    "Make sure your output plan is logically correct, concise, and sufficient to guide final problem solving.\n",
    "Now, output your refined plan below:\n",
    "\"\"\"\n",
    "\n",
    "    elif role == \"judger\":\n",
    "        task = getattr(args, \"task\", None)\n",
    "\n",
    "        if task in [\"gsm8k\", \"aime2024\", \"aime2025\"]:\n",
    "            user_content = f\"\"\"\n",
    "Target Question: {question}\n",
    "\n",
    "You are the final solver agent in a sequential multi-agent system (planner -> critic -> refiner -> solver).\n",
    "You are provided with the Refiner Agent's plan as reference.\n",
    "\n",
    "Refined Plan from Previous Agents:\n",
    "{ctx}\n",
    "\n",
    "The plan might contain irrelevant or incorrect contents. Ignore them if they are not helpful for solving the target question.\n",
    "\n",
    "You must reason step-by-step to solve the **provided Target Question** without outputting other irrelevant information.\n",
    "\n",
    "Now, reason step by step and output the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\"\"\"\n",
    "\n",
    "        elif task in [\"arc_easy\", \"arc_challenge\", \"gpqa\", \"medqa\"]:\n",
    "            user_content = f\"\"\"\n",
    "Target Question: {question}\n",
    "\n",
    "You are the final solver agent in a sequential multi-agent system (planner -> critic -> refiner -> solver).\n",
    "You are provided with the Refiner Agent's plan as reference.\n",
    "\n",
    "Refined Plan from Previous Agents:\n",
    "{ctx}\n",
    "\n",
    "The plan might contain irrelevant or incorrect contents. Ignore them if they are not helpful for solving the target question.\n",
    "\n",
    "You must reason step-by-step to solve the **provided Target Question** without outputting other irrelevant information.\n",
    "Your final answer must be selected from A,B,C,D. For example \\\\boxed{{A}}. Do not add any other contents inside the box.\n",
    "\n",
    "Now, reason step by step and output the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\"\"\"\n",
    "\n",
    "        elif task in [\"mbppplus\", \"humanevalplus\"]:\n",
    "            user_content = f\"\"\"\n",
    "Target Question: {question}\n",
    "\n",
    "You are the final solver agent in a sequential multi-agent system (planner -> critic -> refiner -> solver).\n",
    "You are provided with the Refiner Agent's plan as reference.\n",
    "\n",
    "Refined Plan from Previous Agents:\n",
    "{ctx}\n",
    "\n",
    "The plan might contain irrelevant or incorrect contents. Ignore them if they are not helpful for solving the target question.\n",
    "\n",
    "You must reason step-by-step to solve the **provided Target Question** without outputting other irrelevant information.\n",
    "You must put all python code as self-contained Python function(s) in markdown code blocks. For example:\n",
    "```python\n",
    "import math\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "```\n",
    "Do not add any other contents inside the markdown code block.\n",
    "\"\"\"\n",
    "            \n",
    "        elif task in [\"winogrande\"]:\n",
    "            user_content = f\"\"\"\n",
    "Target Question: {question}\n",
    "\n",
    "You are the final solver agent in a sequential multi-agent system (planner -> critic -> refiner -> solver).\n",
    "You are provided with the Refiner Agent's plan as reference.\n",
    "\n",
    "Refined Plan from Previous Agents:\n",
    "{ctx}\n",
    "\n",
    "The plan might contain irrelevant or incorrect contents. Ignore them if they are not helpful for solving the target question.\n",
    "\n",
    "You must reason step-by-step to solve the **provided Target Question** without outputting other irrelevant information.\n",
    "Your final answer must be selected from 1 and 2. For example \\\\boxed{{1}} or \\\\boxed{{2}}. Do not add any other contents inside the box.\n",
    "\n",
    "Now, reason step by step and output the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\"\"\"\n",
    "        else:\n",
    "            user_content = f\"\"\"\n",
    "Target Question: {question}\n",
    "\n",
    "You are the final solver agent in a sequential multi-agent system (planner -> critic -> refiner -> solver).\n",
    "You are provided with the Refiner Agent's plan as reference.\n",
    "\n",
    "Refined Plan from Previous Agents:\n",
    "{ctx}\n",
    "\n",
    "The plan might contain irrelevant or incorrect contents. Ignore them if they are not helpful for solving the target question.\n",
    "\n",
    "You must reason step-by-step to solve the **provided Target Question** without outputting other irrelevant information.\n",
    "\n",
    "Now, reason step by step and present your final answer clearly at the end.\n",
    "\"\"\"\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_agent_messages_hierarchical_text_mas(role: str, question: str, context: str = \"\", method=None, args=None):\n",
    "\n",
    "    system_message = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "    \n",
    "    assert method in [\"text_mas\"], \"this prompt only for text_mas method\"\n",
    "    assert \"qwen\" in args.model_name.lower(), \"this prompt only for qwen models\"\n",
    "    \n",
    "    if args.task in ['gsm8k', 'aime2024', 'aime2025']:\n",
    "        if role == \"planner\":\n",
    "            user_content = f\"\"\"\n",
    "You are a math agent. Given the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "    \n",
    "        elif role == \"critic\":\n",
    "            user_content = f\"\"\"\n",
    "You are a science agent. Given the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\n",
    "Input Question: {question}     \n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "    \n",
    "        elif role == \"refiner\":\n",
    "            user_content = f\"\"\"\n",
    "You are a code agent. Given the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:       \n",
    "\"\"\"\n",
    "        elif role == \"judger\":\n",
    "            user_content = f\"\"\"\n",
    "You are a task summarizer. Given the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\n",
    "Content from Previous Agent:\n",
    "{context[:args.text_mas_context_length]}\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "\n",
    "    elif args.task in [\"arc_easy\", \"arc_challenge\", \"gpqa\", \"medqa\"]:\n",
    "        if role == \"planner\":\n",
    "            user_content = f\"\"\"\n",
    "You are a math agent. Given the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "    \n",
    "        elif role == \"critic\":\n",
    "            user_content = f\"\"\"\n",
    "You are a science agent. Given the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\n",
    "Input Question: {question}     \n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "    \n",
    "        elif role == \"refiner\":\n",
    "            user_content = f\"\"\"\n",
    "You are a code agent. Given the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:       \n",
    "\"\"\"\n",
    "        elif role == \"judger\":\n",
    "\n",
    "            user_content = f\"\"\"\n",
    "You are a task summarizer. Given the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\n",
    "Content from Previous Agent:\n",
    "{context[:args.text_mas_context_length]}\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "\n",
    "    elif args.task in [\"mbppplus\", \"humanevalplus\"]:\n",
    "        \n",
    "        if role == \"planner\":\n",
    "            user_content = f\"\"\"\n",
    "You are a math agent. You must put all python code as self-contained Python function in markdown code blocks. For example ```python\n",
    "import needed_library\n",
    "def FUNC_NAME(a, b):\n",
    "    return a + b```. Do not add any other contents inside the markdown code block. \n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "        elif role == \"critic\":\n",
    "            user_content = f\"\"\"\n",
    "You are a science agent. You must put all python code as self-contained Python function in markdown code blocks. For example ```python\n",
    "import needed_library\n",
    "def FUNC_NAME(a, b):\n",
    "    return a + b```. Do not add any other contents inside the markdown code block. \n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "        elif role == \"refiner\":\n",
    "            user_content = f\"\"\"\n",
    "You are a code agent. You must put all python code as self-contained Python function in markdown code blocks. For example ```python\n",
    "import needed_library\n",
    "def FUNC_NAME(a, b):\n",
    "    return a + b```. Do not add any other contents inside the markdown code block. \n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "        elif role == \"judger\":\n",
    "            user_content = f\"\"\"\n",
    "You are a task summarizer. Given the final answer in markdown python code block.\n",
    "\n",
    "Content from Previous Agent:\n",
    "{context[:args.text_mas_context_length]}\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "\n",
    "    elif args.task in [\"winogrande\"]:\n",
    "        if role == \"planner\":\n",
    "            user_content = f\"\"\"\n",
    "You are a math agent. Given the input question, reason step-by-step and put the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\"Your final answer must be selected from 1 and 2. For example \\\\boxed{{1}} or \\\\boxed{{2}}. Do not add any other contents inside the box.\"\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "    \n",
    "        elif role == \"critic\":\n",
    "            user_content = f\"\"\"\n",
    "You are a science agent. Given the input question, reason step-by-step and put the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\"Your final answer must be selected from 1 and 2. For example \\\\boxed{{1}} or \\\\boxed{{2}}. Do not add any other contents inside the box.\"\n",
    "\n",
    "Input Question: {question}     \n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "    \n",
    "        elif role == \"refiner\":\n",
    "            user_content = f\"\"\"\n",
    "You are a code agent. Given the input question, reason step-by-step and put the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\"Your final answer must be selected from 1 and 2. For example \\\\boxed{{1}} or \\\\boxed{{2}}. Do not add any other contents inside the box.\"\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:       \n",
    "\"\"\"\n",
    "        elif role == \"judger\":\n",
    "            user_content = f\"\"\"\n",
    "You are a task summarizer. Given the input question and responses from previous agents as reference, reason step-by-step and put the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\n",
    "Content from Previous Agent:\n",
    "{context[:args.text_mas_context_length]}\n",
    "\n",
    "\"Your final answer must be selected from 1 and 2. For example \\\\boxed{{1}} or \\\\boxed{{2}}. Do not add any other contents inside the box.\"\n",
    "\n",
    "Input Question: {question}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_agent_messages_single_agent(question: str, args=None):\n",
    "\n",
    "    system_message = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "\n",
    "    assert args.method in [\"baseline\"], \"this prompt only for baseline method (single agent)\"\n",
    "    assert \"qwen\" in args.model_name.lower(), \"this prompt only for qwen models\"\n",
    "\n",
    "    task = args.task\n",
    "\n",
    "    if task in [\"gsm8k\", \"aime2024\", \"aime2025\"]:\n",
    "        user_content = f\"\"\"\n",
    "Target Question: {question}\n",
    "\n",
    "You are a helpful assistant.\n",
    "\n",
    "You must reason step-by-step to solve the **provided Target Question** without outputting other irrelevant information.\n",
    "\n",
    "Now, reason step by step and output the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\"\"\"\n",
    "\n",
    "    elif task in [\"arc_easy\", \"arc_challenge\", \"gpqa\", \"medqa\"]:\n",
    "        user_content = f\"\"\"\n",
    "Target Question: {question}\n",
    "\n",
    "You are a helpful assistant.\n",
    "\n",
    "You must reason step-by-step to solve the **provided Target Question** without outputting other irrelevant information.\n",
    "Your final answer must be selected from A,B,C,D. For example \\\\boxed{{A}}. Do not add any other contents inside the box.\n",
    "\n",
    "Now, reason step by step and output the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\"\"\"\n",
    "\n",
    "    elif task in [\"mbppplus\", \"humanevalplus\"]:\n",
    "        user_content = f\"\"\"\n",
    "Target Question: {question}\n",
    "\n",
    "You must put all python code as self-contained Python function(s) in markdown code blocks. For example:\n",
    "```python\n",
    "import math\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "```\n",
    "Do not add any other contents inside the markdown code block.\n",
    "Now, reason step by step and output the final answer:\n",
    "\"\"\"\n",
    "\n",
    "    elif task in [\"winogrande\"]:\n",
    "        user_content = f\"\"\"\n",
    "Target Question: {question}\n",
    "\n",
    "You are a helpful assistant.\n",
    "\n",
    "You must reason step-by-step to solve the **provided Target Question** without outputting other irrelevant information.\n",
    "Your final answer must be selected from 1 and 2. For example \\\\boxed{{1}} or \\\\boxed{{2}}. Do not add any other contents inside the box.\n",
    "\n",
    "Now, reason step by step and output the final answer inside \\\\boxed{{YOUR_FINAL_ANSWER}}.\n",
    "\"\"\"\n",
    "\n",
    "    else:\n",
    "        user_content = f\"\"\"\n",
    "Question: {question}\n",
    "\n",
    "You are a helpful assistant.\n",
    "\n",
    "You must reason step-by-step to solve the question without outputting other irrelevant information.\n",
    "Present your reasoning, and then clearly state your final answer at the end.\n",
    "\"\"\"\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c0f19a",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e45b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "def auto_device(device: Optional[str] = None) -> torch.device:\n",
    "    if device is not None:\n",
    "        return torch.device(device)\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "# this is to extract answer in \\boxed{}\n",
    "def extract_gsm8k_answer(text: str) -> Optional[str]:\n",
    "    boxes = re.findall(r\"\\\\boxed\\{([^}]*)\\}\", text)\n",
    "    if boxes:\n",
    "        content = boxes[-1]\n",
    "        number = re.search(r\"[-+]?\\d+(?:\\.\\d+)?\", content)\n",
    "        return number.group(0) if number else content.strip()\n",
    "\n",
    "    numbers = re.findall(r\"[-+]?\\d+(?:\\.\\d+)?\", text)\n",
    "    if numbers:\n",
    "        return numbers[-1]\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_gold(text: str) -> Optional[str]:\n",
    "    match = re.search(r\"####\\s*([-+]?\\d+(?:\\.\\d+)?)\", text)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "def normalize_answer(ans: Optional[str]) -> Optional[str]:\n",
    "    if ans is None:\n",
    "        return None\n",
    "    return ans.strip().lower()\n",
    "\n",
    "\n",
    "def extract_markdown_python_block(text: str) -> Optional[str]:\n",
    "    pattern = r\"```python(.*?)```\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "    if matches:\n",
    "        return matches[-1].strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "# to run python\n",
    "import traceback\n",
    "from multiprocessing import Process, Manager\n",
    "def run_with_timeout(code, timeout):\n",
    "    def worker(ns, code):\n",
    "        try:\n",
    "            local_ns = {}\n",
    "            exec(code, local_ns)\n",
    "            ns['ok'] = True\n",
    "            ns['error'] = None\n",
    "        except Exception:\n",
    "            ns['ok'] = False\n",
    "            ns['error'] = traceback.format_exc()\n",
    "    with Manager() as manager:\n",
    "        ns = manager.dict()\n",
    "        p = Process(target=worker, args=(ns, code))\n",
    "        p.start()\n",
    "        p.join(timeout)\n",
    "        if p.is_alive():\n",
    "            p.terminate()\n",
    "            ns['ok'] = False\n",
    "            ns['error'] = f\"TimeoutError: Execution exceeded {timeout} seconds\"\n",
    "        return ns.get('ok', False), ns.get('error', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d29dc",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7039397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_pad_token(tokenizer: AutoTokenizer) -> None:\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        if tokenizer.eos_token is not None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        else:\n",
    "            tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "\n",
    "\n",
    "def _past_length(past_key_values: Optional[Tuple]) -> int:\n",
    "    if not past_key_values:\n",
    "        return 0\n",
    "    k = past_key_values[0][0]\n",
    "    return k.shape[-2]\n",
    "\n",
    "\n",
    "class ModelWrapper:\n",
    "    def __init__(self, model_name: str, device: torch.device, use_vllm: bool = False, args = None):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.use_vllm = use_vllm and _HAS_VLLM\n",
    "        self.vllm_engine = None\n",
    "        self.latent_space_realign = bool(getattr(args, \"latent_space_realign\", False)) if args else False\n",
    "        self._latent_realign_matrices: Dict[int, Tuple[torch.Tensor, torch.Tensor]] = {}\n",
    "        self.args = args\n",
    "\n",
    "        # for ablation\n",
    "        self.pre_aligned = None\n",
    "\n",
    "        if self.use_vllm:\n",
    "            \n",
    "            tp_size = max(1, int(getattr(args, \"tensor_parallel_size\", 1)))\n",
    "            gpu_util = float(getattr(args, \"gpu_memory_utilization\", 0.9))\n",
    "            \n",
    "            print(f\"[vLLM] Using vLLM backend for model {model_name}\")\n",
    "            if args.enable_prefix_caching and args.method == \"latent_mas\": \n",
    "                self.vllm_engine = LLM(model=model_name, tensor_parallel_size=tp_size, gpu_memory_utilization=gpu_util, enable_prefix_caching=True, enable_prompt_embeds=True)\n",
    "            else:\n",
    "                self.vllm_engine = LLM(model=model_name, tensor_parallel_size=tp_size, gpu_memory_utilization=gpu_util)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "            \n",
    "            use_second_hf = bool(getattr(args, \"use_second_HF_model\", False)) if args else False\n",
    "            if use_second_hf:\n",
    "                self.HF_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    torch_dtype=(torch.bfloat16 if torch.cuda.is_available() else torch.float32),\n",
    "                ).to(args.device2).eval() \n",
    "                self.embedding_layer = self.HF_model.get_input_embeddings()\n",
    "                self.HF_device = args.device2\n",
    "                # if self.latent_space_realign:\n",
    "                self._ensure_latent_realign_matrix(self.HF_model, torch.device(self.HF_device), args)\n",
    "            elif self.latent_space_realign:\n",
    "                raise ValueError(\"latent_space_realign requires --use_second_HF_model when using vLLM backend.\")\n",
    "            _ensure_pad_token(self.tokenizer)\n",
    "            return  # skip loading transformers model\n",
    "\n",
    "        # fallback: normal transformers path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        _ensure_pad_token(self.tokenizer)\n",
    "        with torch.no_grad():\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=(torch.bfloat16 if torch.cuda.is_available() else torch.float32),\n",
    "            )\n",
    "        if len(self.tokenizer) != self.model.get_input_embeddings().weight.shape[0]:\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        if hasattr(self.model.config, \"use_cache\"):\n",
    "            self.model.config.use_cache = True\n",
    "        if self.latent_space_realign:\n",
    "            self._ensure_latent_realign_matrix(self.model, self.device, args)\n",
    "\n",
    "    def render_chat(self, messages: List[Dict], add_generation_prompt: bool = True) -> str:\n",
    "        tpl = getattr(self.tokenizer, \"chat_template\", None)\n",
    "        if tpl:\n",
    "            return self.tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=add_generation_prompt\n",
    "            )\n",
    "        segments = []\n",
    "        for message in messages:\n",
    "            role = message.get(\"role\", \"user\")\n",
    "            content = message.get(\"content\", \"\")\n",
    "            segments.append(f\"<|{role}|>\\n{content}\\n</|{role}|>\")\n",
    "        if add_generation_prompt:\n",
    "            segments.append(\"<|assistant|>\")\n",
    "        return \"\\n\".join(segments)\n",
    "\n",
    "    def prepare_chat_input(\n",
    "        self, messages: List[Dict], add_generation_prompt: bool = True\n",
    "    ) -> Tuple[str, torch.Tensor, torch.Tensor, List[str]]:\n",
    "        prompt_text = self.render_chat(messages, add_generation_prompt=add_generation_prompt)\n",
    "        encoded = self.tokenizer(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        input_ids = encoded[\"input_ids\"].to(self.device)\n",
    "        attention_mask = encoded[\"attention_mask\"].to(self.device)\n",
    "        active_ids = input_ids[0][attention_mask[0].bool()].tolist()\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(active_ids)\n",
    "        return prompt_text, input_ids, attention_mask, tokens\n",
    "\n",
    "    def prepare_chat_batch(\n",
    "        self,\n",
    "        batch_messages: List[List[Dict]],\n",
    "        add_generation_prompt: bool = True,\n",
    "    ) -> Tuple[List[str], torch.Tensor, torch.Tensor, List[List[str]]]:\n",
    "        prompts: List[str] = []\n",
    "        for messages in batch_messages:\n",
    "            prompts.append(self.render_chat(messages, add_generation_prompt=add_generation_prompt))\n",
    "        encoded = self.tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        input_ids = encoded[\"input_ids\"].to(self.device)\n",
    "        attention_mask = encoded[\"attention_mask\"].to(self.device)\n",
    "        tokens_batch: List[List[str]] = []\n",
    "        for ids_row, mask_row in zip(input_ids, attention_mask):\n",
    "            active_ids = ids_row[mask_row.bool()].tolist()\n",
    "            tokens_batch.append(self.tokenizer.convert_ids_to_tokens(active_ids))\n",
    "        return prompts, input_ids, attention_mask, tokens_batch\n",
    "\n",
    "    def vllm_generate_text_batch(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        *,\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.95,\n",
    "    ) -> List[str]:\n",
    "        if not self.vllm_engine:\n",
    "            raise RuntimeError(\"vLLM engine not initialized. Pass use_vllm=True to ModelWrapper.\")\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_new_tokens,\n",
    "        )\n",
    "        outputs = self.vllm_engine.generate(prompts, sampling_params)\n",
    "        generations = [out.outputs[0].text.strip() for out in outputs]\n",
    "        return generations\n",
    "    \n",
    "    def _build_latent_realign_matrix(self, model, device, args) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_embeds = model.get_input_embeddings() if hasattr(model, \"get_input_embeddings\") else None\n",
    "        output_embeds = model.get_output_embeddings() if hasattr(model, \"get_output_embeddings\") else None\n",
    "        if output_embeds is None:\n",
    "            output_embeds = getattr(model, \"lm_head\", None)\n",
    "        if (\n",
    "            input_embeds is None\n",
    "            or output_embeds is None\n",
    "            or not hasattr(input_embeds, \"weight\")\n",
    "            or not hasattr(output_embeds, \"weight\")\n",
    "        ):\n",
    "            raise RuntimeError(\"Cannot build latent realignment matrix: embedding weights not accessible.\")\n",
    "        input_weight = input_embeds.weight.detach().to(device=device, dtype=torch.float32)\n",
    "        output_weight = output_embeds.weight.detach().to(device=device, dtype=torch.float32)\n",
    "        gram = torch.matmul(output_weight.T, output_weight)\n",
    "        reg = 1e-5 * torch.eye(gram.shape[0], device=gram.device, dtype=gram.dtype)\n",
    "        gram = gram + reg\n",
    "        rhs = torch.matmul(output_weight.T, input_weight)\n",
    "        realign_matrix = torch.linalg.solve(gram, rhs)\n",
    "        target_norm = input_weight.norm(dim=1).mean().detach()\n",
    "\n",
    "        if self.args.latent_space_realign:\n",
    "            pass\n",
    "        else:\n",
    "            # keep the matrix, for further normalization\n",
    "            realign_matrix = torch.eye(realign_matrix.shape[0], device=realign_matrix.device, dtype=realign_matrix.dtype)\n",
    "\n",
    "        return realign_matrix, target_norm\n",
    "\n",
    "    def _ensure_latent_realign_matrix(self, model, device, args) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        key = id(model)\n",
    "        info = self._latent_realign_matrices.get(key)\n",
    "        target_device = torch.device(device)\n",
    "\n",
    "        if info is None:\n",
    "            matrix, target_norm = self._build_latent_realign_matrix(model, target_device, args)\n",
    "        else:\n",
    "            matrix, target_norm = info\n",
    "            if matrix.device != target_device:\n",
    "                matrix = matrix.to(target_device)\n",
    "\n",
    "        target_norm = target_norm.to(device=target_device, dtype=matrix.dtype) if isinstance(target_norm, torch.Tensor) else torch.as_tensor(target_norm, device=target_device, dtype=matrix.dtype)\n",
    "        self._latent_realign_matrices[key] = (matrix, target_norm)\n",
    "\n",
    "        return matrix, target_norm\n",
    "\n",
    "    def _apply_latent_realignment(self, hidden: torch.Tensor, model: torch.nn.Module) -> torch.Tensor:\n",
    "        matrix, target_norm = self._ensure_latent_realign_matrix(model, hidden.device, self.args)\n",
    "        hidden_fp32 = hidden.to(torch.float32)\n",
    "        aligned = torch.matmul(hidden_fp32, matrix)\n",
    "\n",
    "        aligned_norm = aligned.norm(dim=-1, keepdim=True).clamp_min(1e-6)\n",
    "        pre_aligned = aligned.detach().clone()\n",
    "        self.pre_aligned = pre_aligned\n",
    "        aligned = aligned * (target_norm / aligned_norm)\n",
    "        return aligned.to(hidden.dtype)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_text_batch(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        *,\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.95,\n",
    "        past_key_values: Optional[Tuple] = None,\n",
    "    ) -> Tuple[List[str], Optional[Tuple]]:\n",
    "        if input_ids.dim() != 2:\n",
    "            raise ValueError(\"input_ids must be 2D with shape [batch, seq_len]\")\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids, device=self.device)\n",
    "        prompt_lengths = attention_mask.sum(dim=1).tolist()\n",
    "        cache_position = None\n",
    "        if past_key_values is not None:\n",
    "            past_len = _past_length(past_key_values)\n",
    "            cache_position = torch.arange(\n",
    "                past_len,\n",
    "                past_len + input_ids.shape[-1],\n",
    "                dtype=torch.long,\n",
    "                device=self.device,\n",
    "            )\n",
    "            if past_len > 0:\n",
    "                past_mask = torch.ones(\n",
    "                    (attention_mask.shape[0], past_len),\n",
    "                    dtype=attention_mask.dtype,\n",
    "                    device=attention_mask.device,\n",
    "                )\n",
    "                attention_mask = torch.cat([past_mask, attention_mask], dim=-1)\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,\n",
    "            past_key_values=past_key_values,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        sequences = outputs.sequences\n",
    "        generations: List[str] = []\n",
    "        for idx, length in enumerate(prompt_lengths):\n",
    "            length = int(length)\n",
    "            generated_ids = sequences[idx, length:]\n",
    "            text = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "            generations.append(text)\n",
    "        return generations, outputs.past_key_values\n",
    "\n",
    "    def tokenize_text(self, text: str) -> torch.Tensor:\n",
    "        return self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"].to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_latent_batch(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        *,\n",
    "        latent_steps: int,\n",
    "        past_key_values: Optional[Tuple] = None,\n",
    "    ) -> Tuple:\n",
    "        if input_ids.dim() != 2:\n",
    "            raise ValueError(\"input_ids must be 2D with shape [batch, seq_len]\")\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids, device=self.device)\n",
    "        else:\n",
    "            attention_mask = attention_mask.to(self.device)\n",
    "\n",
    "        if past_key_values is not None:\n",
    "            past_len = _past_length(past_key_values)\n",
    "            if past_len > 0:\n",
    "                past_mask = torch.ones(\n",
    "                    (attention_mask.shape[0], past_len),\n",
    "                    dtype=attention_mask.dtype,\n",
    "                    device=attention_mask.device,\n",
    "                )\n",
    "                attention_mask = torch.cat([past_mask, attention_mask], dim=-1)\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        past = outputs.past_key_values\n",
    "\n",
    "        e_t = outputs.hidden_states[0][:, -1, :]          # [B, D]\n",
    "        last_hidden = outputs.hidden_states[-1][:, -1, :] # [B, D]\n",
    "        h_t = last_hidden.detach().clone()\n",
    "\n",
    "        e_t_plus_1 = None\n",
    "        latent_vecs_all: List[torch.Tensor] = []\n",
    "        latent_vecs_all.append(e_t.detach().clone())\n",
    "\n",
    "        for step in range(latent_steps):\n",
    "\n",
    "            source_model = self.HF_model if hasattr(self, \"HF_model\") else self.model\n",
    "            latent_vec = self._apply_latent_realignment(last_hidden, source_model)\n",
    "\n",
    "            latent_vecs_all.append(latent_vec.detach().clone())\n",
    "\n",
    "            if step == 0:\n",
    "                e_t_plus_1 = latent_vec.detach().clone()\n",
    "            \n",
    "            latent_embed = latent_vec.unsqueeze(1)\n",
    "\n",
    "            past_len = _past_length(past)\n",
    "            latent_mask = torch.ones(\n",
    "                (latent_embed.shape[0], past_len + 1),\n",
    "                dtype=torch.long,\n",
    "                device=self.device,\n",
    "            )\n",
    "            outputs = self.model(\n",
    "                inputs_embeds=latent_embed,\n",
    "                attention_mask=latent_mask,\n",
    "                past_key_values=past,\n",
    "                use_cache=True,\n",
    "                output_hidden_states=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            past = outputs.past_key_values\n",
    "            last_hidden = outputs.hidden_states[-1][:, -1, :]\n",
    "\n",
    "        return past\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_latent_batch_hidden_state(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        *,\n",
    "        latent_steps: int,\n",
    "        past_key_values: Optional[Tuple] = None,\n",
    "    ) -> Tuple:\n",
    "        if input_ids.dim() != 2:\n",
    "            raise ValueError(\"input_ids must be 2D with shape [batch, seq_len]\")\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids, device=self.HF_device)\n",
    "        else:\n",
    "            attention_mask = attention_mask.to(self.HF_device)\n",
    "        if past_key_values is not None:\n",
    "            past_len = _past_length(past_key_values)\n",
    "            if past_len > 0:\n",
    "                past_mask = torch.ones(\n",
    "                    (attention_mask.shape[0], past_len),\n",
    "                    dtype=attention_mask.dtype,\n",
    "                    device=attention_mask.device,\n",
    "                )\n",
    "                attention_mask = torch.cat([past_mask, attention_mask], dim=-1)\n",
    "        outputs = self.HF_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        past = outputs.past_key_values\n",
    "        last_hidden = outputs.hidden_states[-1][:, -1, :]\n",
    "        \n",
    "        curr_output_embedding = [] \n",
    "        curr_output_embedding.append(outputs.hidden_states[0])  # input embedding\n",
    "        \n",
    "        \n",
    "        for _ in range(latent_steps):\n",
    "\n",
    "            source_model = self.HF_model if hasattr(self, \"HF_model\") else self.model\n",
    "            latent_vec = self._apply_latent_realignment(last_hidden, source_model)\n",
    "            latent_embed = latent_vec.unsqueeze(1)\n",
    "            past_len = _past_length(past)\n",
    "            latent_mask = torch.ones(\n",
    "                (latent_embed.shape[0], past_len + 1),\n",
    "                dtype=torch.long,\n",
    "                device=latent_embed.device,\n",
    "            )\n",
    "            outputs = self.HF_model(\n",
    "                inputs_embeds=latent_embed,\n",
    "                attention_mask=latent_mask,\n",
    "                past_key_values=past,\n",
    "                use_cache=True,\n",
    "                output_hidden_states=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            past = outputs.past_key_values\n",
    "            last_hidden = outputs.hidden_states[-1][:, -1, :]\n",
    "\n",
    "            curr_output_embedding.append(latent_embed.detach())\n",
    "\n",
    "        return past, torch.cat(curr_output_embedding, dim=1) # Output input embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba5b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gsm8k(split: str = \"test\", cache_dir: Optional[str] = None) -> Iterable[Dict]:\n",
    "    ds = load_dataset(\"gsm8k\", \"main\", split=split, cache_dir=cache_dir)\n",
    "    for item in ds:\n",
    "        question = item[\"question\"].strip()\n",
    "        solution = item[\"answer\"]\n",
    "        gold = normalize_answer(extract_gold(solution))\n",
    "        yield {\n",
    "            \"question\": question,\n",
    "            \"solution\": solution,\n",
    "            \"gold\": gold,\n",
    "        }\n",
    "\n",
    "\n",
    "def load_aime2025(split: str = \"train\", cache_dir: Optional[str] = None) -> Iterable[Dict]:\n",
    "    ds = load_dataset(\"yentinglin/aime_2025\", split=split, cache_dir=cache_dir)\n",
    "    for item in ds:\n",
    "        problem = item[\"problem\"].strip()\n",
    "        answer = str(item[\"answer\"]).strip()\n",
    "        gold = normalize_answer(answer)\n",
    "        yield {\n",
    "            \"question\": problem,\n",
    "            \"solution\": answer,\n",
    "            \"gold\": gold,\n",
    "        }\n",
    "\n",
    "\n",
    "def load_aime2024(split: str = \"train\", cache_dir: Optional[str] = None) -> Iterable[Dict]:\n",
    "    ds = load_dataset(\"HuggingFaceH4/aime_2024\", split=split, cache_dir=cache_dir)\n",
    "    for item in ds:\n",
    "        problem = item[\"problem\"].strip()\n",
    "        answer = str(item[\"answer\"]).strip()\n",
    "        gold = normalize_answer(answer)\n",
    "        yield {\n",
    "            \"question\": problem,\n",
    "            \"solution\": answer,\n",
    "            \"gold\": gold,\n",
    "        }\n",
    "\n",
    "\n",
    "def load_gpqa_diamond(split: str = \"test\", cache_dir: Optional[str] = None) -> Iterable[Dict]:\n",
    "    ds = load_dataset(\"fingertap/GPQA-Diamond\", split=split, cache_dir=cache_dir)\n",
    "    for item in ds:\n",
    "        question = item[\"question\"].strip()\n",
    "        answer = item[\"answer\"].strip()\n",
    "        gold = normalize_answer(answer)\n",
    "        yield {\n",
    "            \"question\": question,\n",
    "            \"solution\": answer,\n",
    "            \"gold\": gold,\n",
    "        }\n",
    "\n",
    "\n",
    "def load_arc_easy(split: str = \"test\", cache_dir: Optional[str] = None) -> Iterable[Dict]:\n",
    "    ds = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\", split=split, cache_dir=cache_dir)\n",
    "    for item in ds:\n",
    "        stem = item[\"question\"].strip()\n",
    "        choices = item[\"choices\"]\n",
    "        labels = choices[\"label\"]\n",
    "        texts = choices[\"text\"]\n",
    "        label_map = {\"1\": \"a\", \"2\": \"b\", \"3\": \"c\", \"4\": \"d\"}\n",
    "\n",
    "        def map_label(l: str) -> str:\n",
    "            s = str(l).strip()\n",
    "            if s in label_map:\n",
    "                return label_map[s]\n",
    "            return s.lower()\n",
    "\n",
    "        # Map choices\n",
    "        formatted_choices = {}\n",
    "        mapped_order = []\n",
    "        for label, text in zip(labels, texts):\n",
    "            mlabel = map_label(label)\n",
    "            formatted_choices[mlabel] = text.strip()\n",
    "            mapped_order.append(mlabel)\n",
    "\n",
    "        ordered_lines = [f\"{lab}: {formatted_choices[lab]}\" for lab in mapped_order]\n",
    "        question = stem + \"\\n\" + \"\\n\".join(ordered_lines)\n",
    "\n",
    "        # Map answers\n",
    "        raw_answer = item.get(\"answerKey\", \"\").strip()\n",
    "        mapped_answer = map_label(raw_answer) if raw_answer else \"\"\n",
    "        gold = normalize_answer(mapped_answer)\n",
    "        yield {\n",
    "            \"question\": question,\n",
    "            \"solution\": mapped_answer,\n",
    "            \"gold\": gold,\n",
    "        }\n",
    "\n",
    "\n",
    "def load_arc_challenge(split: str = \"test\", cache_dir: Optional[str] = None) -> Iterable[Dict]:\n",
    "    ds = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=split, cache_dir=cache_dir)\n",
    "    for item in ds:\n",
    "        stem = item[\"question\"].strip()\n",
    "        choices = item[\"choices\"]\n",
    "        labels = choices[\"label\"]\n",
    "        texts = choices[\"text\"]\n",
    "        label_map = {\"1\": \"a\", \"2\": \"b\", \"3\": \"c\", \"4\": \"d\"}\n",
    "\n",
    "        def map_label(l: str) -> str:\n",
    "            s = str(l).strip()\n",
    "            if s in label_map:\n",
    "                return label_map[s]\n",
    "            return s.lower()\n",
    "\n",
    "        formatted_choices = {}\n",
    "        mapped_order = []\n",
    "        for label, text in zip(labels, texts):\n",
    "            mlabel = map_label(label)\n",
    "            formatted_choices[mlabel] = text.strip()\n",
    "            mapped_order.append(mlabel)\n",
    "\n",
    "        ordered_lines = [f\"{lab}: {formatted_choices[lab]}\" for lab in mapped_order]\n",
    "        question = stem + \"\\n\" + \"\\n\".join(ordered_lines)\n",
    "\n",
    "        raw_answer = item.get(\"answerKey\", \"\").strip()\n",
    "        mapped_answer = map_label(raw_answer) if raw_answer else \"\"\n",
    "        gold = normalize_answer(mapped_answer)\n",
    "        yield {\n",
    "            \"question\": question,\n",
    "            \"solution\": mapped_answer,\n",
    "            \"gold\": gold,\n",
    "        }\n",
    "\n",
    "\n",
    "def load_winogrande(\n",
    "    split: str = \"validation\",\n",
    "    subset: str = \"winogrande_debiased\",\n",
    "    cache_dir: Optional[str] = None,\n",
    ") -> Iterable[Dict]:\n",
    "    ds = load_dataset(\"allenai/winogrande\", subset, split=split, cache_dir=cache_dir)\n",
    "    for item in ds:\n",
    "        ask_str = 'Pickout proper choice that fits the _ in the following sentence:'\n",
    "        sentence = item[\"sentence\"].strip()\n",
    "        option1 = str(item[\"option1\"]).strip()\n",
    "        option2 = str(item[\"option2\"]).strip()\n",
    "        question = f\"{ask_str}\\n{sentence}\\n1: {option1}\\n2: {option2}\"\n",
    "        answer = str(item[\"answer\"])\n",
    "        gold = normalize_answer(answer)\n",
    "        yield {\n",
    "            \"question\": question,\n",
    "            \"solution\": answer,\n",
    "            \"gold\": gold,\n",
    "        }\n",
    "\n",
    "\n",
    "def load_mbppplus(\n",
    "    split: str = \"test\",\n",
    "    subset: str = None,\n",
    "    cache_dir: Optional[str] = None,\n",
    ") -> Iterable[Dict]:\n",
    "    ds = load_dataset(\"evalplus/mbppplus\", subset, split=split, cache_dir=cache_dir)\n",
    "    for item in ds:\n",
    "        question = f\"\"\"Please provide a self-contained Python script that solves the following problem in a markdown code block:\\n```python\\nYOUR_PYTHON_CODE\\n```:\n",
    "{item[\"prompt\"]}\n",
    "Your answer will be tested on test cases like:\n",
    "{item[\"test_list\"][0]}\n",
    "{item[\"test_list\"][1]}\n",
    "{item[\"test_list\"][2]}\n",
    "\"\"\"\n",
    "\n",
    "        answer = str(item[\"test\"])\n",
    "        gold = answer\n",
    "        yield {\n",
    "            \"question\": question,\n",
    "            \"solution\": answer,\n",
    "            \"gold\": gold,\n",
    "        }\n",
    "\n",
    "\n",
    "def load_humanevalplus(\n",
    "    split: str = \"test\",\n",
    "    subset: str = None,\n",
    "    cache_dir: Optional[str] = None,\n",
    ") -> Iterable[Dict]:\n",
    "    ds = load_dataset(\"evalplus/humanevalplus\", subset, split=split, cache_dir=cache_dir)\n",
    "    for item in ds:\n",
    "        question = f\"\"\"Please provide a self-contained Python script that solves the following problem in a markdown code block:\\n```python\\nYOUR_PYTHON_CODE\\n```:\n",
    "{item[\"prompt\"]}\n",
    "\"\"\"\n",
    "        raw_answer = str(item[\"test\"])\n",
    "        answer = raw_answer.replace('candidate', item['entry_point'])\n",
    "        answer += f'\\n\\ncheck({item[\"entry_point\"]})'\n",
    "        gold = answer\n",
    "        yield {\n",
    "            \"question\": question,\n",
    "            \"solution\": answer,\n",
    "            \"gold\": gold,\n",
    "        }\n",
    "\n",
    "\n",
    "# qa data from https://github.com/lupantech/AgentFlow/tree/main\n",
    "from typing import Iterable, Dict, Optional\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_medqa(split=None, subset=None, cache_dir=None):\n",
    "\n",
    "    ds = load_dataset(\"json\", data_files=\"./data/medqa.json\", split='train')\n",
    "    for item in ds:\n",
    "        question = item[\"query\"]\n",
    "        raw_answer = str(item[\"answer\"])\n",
    "\n",
    "        choice_map = {\"0\":\"A\", \"1\":\"B\", \"2\":\"C\", \"3\":\"D\"}\n",
    "\n",
    "        for idx, op in enumerate(item['options']):\n",
    "            if raw_answer in op:\n",
    "                answer = choice_map[str(idx)].lower()\n",
    "                break\n",
    "\n",
    "        gold = normalize_answer(answer)\n",
    "\n",
    "        yield {\n",
    "            \"question\": question,\n",
    "            \"solution\": answer,\n",
    "            \"gold\": gold,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5a85a",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a98472",
   "metadata": {},
   "source": [
    "### Models Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86a0f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Agent:\n",
    "    name: str\n",
    "    role: str\n",
    "\n",
    "\n",
    "def default_agents() -> List[Agent]:\n",
    "    return [\n",
    "        Agent(name=\"Planner\", role=\"planner\"),\n",
    "        Agent(name=\"Critic\", role=\"critic\"),\n",
    "        Agent(name=\"Refiner\", role=\"refiner\"),\n",
    "        Agent(name=\"Judger\", role=\"judger\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "__all__ = [\"Agent\", \"default_agents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ae58c",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90d928fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineMethod:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: ModelWrapper,\n",
    "        *,\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.95,\n",
    "        generate_bs: int = 1,\n",
    "        use_vllm: bool = False,\n",
    "        args=None,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        self.generate_bs = max(1, generate_bs)\n",
    "        self.use_vllm = use_vllm\n",
    "        self.method_name = \"baseline\"\n",
    "        self.args = args\n",
    "        self.task = args.task\n",
    "\n",
    "    def run_batch(self, items: List[Dict]) -> List[Dict]:\n",
    "        if len(items) > self.generate_bs:\n",
    "            raise ValueError(\"Batch size exceeds configured generate_bs\")\n",
    "        batch_messages = [\n",
    "            build_agent_messages_single_agent(question=item[\"question\"], args=self.args)\n",
    "            for item in items\n",
    "        ]\n",
    "        prompts, input_ids, attention_mask, tokens_batch = self.model.prepare_chat_batch(\n",
    "            batch_messages, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        if self.use_vllm:\n",
    "            generated_batch = self.model.vllm_generate_text_batch(\n",
    "                prompts,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                temperature=self.temperature,\n",
    "                top_p=self.top_p,\n",
    "            )\n",
    "        else:\n",
    "            generated_batch, _ = self.model.generate_text_batch(\n",
    "                input_ids,\n",
    "                attention_mask,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                temperature=self.temperature,\n",
    "                top_p=self.top_p,\n",
    "            )\n",
    "\n",
    "        results: List[Dict] = []\n",
    "        \n",
    "        for idx, item in enumerate(items):\n",
    "            generated_text = generated_batch[idx]\n",
    "            \n",
    "            if self.task in ['mbppplus', 'humanevalplus']:\n",
    "                pred = extract_markdown_python_block(generated_text)\n",
    "                gold = item.get(\"gold\", \"\")\n",
    "\n",
    "                if pred is None:\n",
    "                    ok = False\n",
    "                    error_msg = \"python error: No python code block found\"\n",
    "                else:\n",
    "                    python_code_to_exe = pred + \"\\n\" + gold\n",
    "                    ok, error_msg = run_with_timeout(python_code_to_exe, timeout=10)\n",
    "                \n",
    "                print(f'=========================================')\n",
    "                print(f'Question {idx}')\n",
    "                print(f'error_msg: {error_msg}')\n",
    "                # print(f'=========================================')\n",
    "\n",
    "            elif self.task in [\"aime2024\", \"aime2025\"]:\n",
    "                pred = normalize_answer(extract_gsm8k_answer(generated_text))\n",
    "                gold = str(item.get(\"gold\", \"\")).strip()\n",
    "                try:\n",
    "                    pred_int = int(pred)\n",
    "                    gold_int = int(gold)\n",
    "                    ok = (pred_int == gold_int)\n",
    "                    error_msg = None\n",
    "                except ValueError:\n",
    "                    ok = False\n",
    "                    error_msg = f'Value error in parsing answer. Pred: {pred}, Gold: {gold}'\n",
    "\n",
    "            else:\n",
    "                pred = normalize_answer(extract_gsm8k_answer(generated_text))\n",
    "                gold = item.get(\"gold\", \"\")\n",
    "                ok = (pred == gold) if (pred and gold) else False\n",
    "                error_msg = None\n",
    "            \n",
    "            mask = attention_mask[idx].bool()\n",
    "            trimmed_ids = input_ids[idx][mask].to(\"cpu\").tolist()\n",
    "            agent_trace = {\n",
    "                \"name\": \"SingleAgent\",\n",
    "                \"role\": \"singleagent\",\n",
    "                \"input\": prompts[idx],\n",
    "                \"input_ids\": trimmed_ids,\n",
    "                \"input_tokens\": tokens_batch[idx],\n",
    "                \"output\": generated_text,\n",
    "            }\n",
    "            results.append(\n",
    "                {\n",
    "                    \"question\": item[\"question\"],\n",
    "                    \"gold\": gold,\n",
    "                    \"solution\": item[\"solution\"],\n",
    "                    \"prediction\": pred,\n",
    "                    \"raw_prediction\": generated_text,\n",
    "                    \"agents\": [agent_trace],\n",
    "                    \"correct\": ok,\n",
    "                }\n",
    "            )\n",
    "        return results\n",
    "\n",
    "    def run_item(self, item: Dict) -> Dict:\n",
    "        return self.run_batch([item])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b48fa9",
   "metadata": {},
   "source": [
    "### TextMAS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b022985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextMASMethod:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: ModelWrapper,\n",
    "        *,\n",
    "        max_new_tokens_each: int = 256,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.95,\n",
    "        generate_bs: int = 1,\n",
    "        args: argparse.Namespace = None,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.max_new_tokens_each = max_new_tokens_each\n",
    "        self.max_new_tokens_judger = max_new_tokens_each\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        self.generate_bs = max(1, generate_bs)\n",
    "        self.agents = default_agents()\n",
    "        self.args = args\n",
    "        self.method_name = \"text_mas\"\n",
    "        self.task = args.task\n",
    "        \n",
    "    def run_batch(self, items: List[Dict]) -> List[Dict]:\n",
    "        if len(items) > self.generate_bs:\n",
    "            raise ValueError(\"Batch size exceeds configured generate_bs\")\n",
    "\n",
    "        batch_size = len(items)\n",
    "        contexts = [\"\" for _ in range(batch_size)]\n",
    "        history_contexts = [\"\" for _ in range(batch_size)]\n",
    "        agent_traces: List[List[Dict]] = [[] for _ in range(batch_size)]\n",
    "        final_texts = [\"\" for _ in range(batch_size)]\n",
    "\n",
    "        for agent in self.agents:\n",
    "\n",
    "            if self.args.prompt == \"hierarchical\":\n",
    "                batch_messages = [\n",
    "                    build_agent_messages_hierarchical_text_mas(\n",
    "                        role=agent.role,\n",
    "                        question=item[\"question\"],\n",
    "                        context=contexts[idx],\n",
    "                        method=self.method_name,\n",
    "                        args=self.args,\n",
    "                    )\n",
    "                    for idx, item in enumerate(items)\n",
    "                ]\n",
    "            else:\n",
    "                batch_messages = [\n",
    "                    build_agent_messages_sequential_text_mas(\n",
    "                        role=agent.role,\n",
    "                        question=item[\"question\"],\n",
    "                        context=contexts[idx],\n",
    "                        method=self.method_name,\n",
    "                        args=self.args,\n",
    "                    )\n",
    "                    for idx, item in enumerate(items)\n",
    "                ]\n",
    "\n",
    "            prompts, input_ids, attention_mask, tokens_batch = self.model.prepare_chat_batch(\n",
    "                batch_messages, add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            if self.model.use_vllm:\n",
    "                generated_texts = self.model.vllm_generate_text_batch(\n",
    "                    prompts,\n",
    "                    max_new_tokens=self.max_new_tokens_each,\n",
    "                    temperature=self.temperature,\n",
    "                    top_p=self.top_p,\n",
    "                )\n",
    "            else:\n",
    "                generated_texts, _ = self.model.generate_text_batch(\n",
    "                    input_ids,\n",
    "                    attention_mask,\n",
    "                    max_new_tokens=self.max_new_tokens_each,\n",
    "                    temperature=self.temperature,\n",
    "                    top_p=self.top_p,\n",
    "                )\n",
    "\n",
    "            agent_name_map_for_prompt_hierarchical = {\n",
    "                \"Planner\": \"Math Agent\",\n",
    "                \"Critic\": \"Science Agent\",\n",
    "                \"Refiner\": \"Code Agent\",\n",
    "                \"Judger\": \"Task Summrizer\",\n",
    "                \"planner\": \"Math Agent\",\n",
    "                \"critic\": \"Science Agent\",\n",
    "                \"refiner\": \"Code Agent\",\n",
    "                \"judger\": \"Task Summrizer\",\n",
    "            }\n",
    "\n",
    "            for idx in range(batch_size):\n",
    "\n",
    "                text_out = generated_texts[idx].strip()\n",
    "\n",
    "                if self.args.prompt == \"hierarchical\":\n",
    "                    formatted_output = f\"[{agent_name_map_for_prompt_hierarchical[agent.name]}]:\\n{text_out}\\n\\n\"\n",
    "                else:\n",
    "                    formatted_output = f\"[{agent.name}]:\\n{text_out}\\n\\n\"\n",
    "\n",
    "                if agent.role != \"judger\":\n",
    "\n",
    "                    contexts[idx] = f\"{contexts[idx]}{formatted_output}\"\n",
    "                    history_contexts[idx] = f\"{history_contexts[idx]}{formatted_output}\"\n",
    "                else:\n",
    "                    final_texts[idx] = text_out\n",
    "                mask = attention_mask[idx].bool()\n",
    "                trimmed_ids = input_ids[idx][mask].to(\"cpu\").tolist()\n",
    "                agent_traces[idx].append(\n",
    "                    {\n",
    "                        \"name\": agent.name,\n",
    "                        \"role\": agent.role,\n",
    "                        \"input\": prompts[idx],\n",
    "                        \"input_ids\": trimmed_ids,\n",
    "                        \"input_tokens\": tokens_batch[idx],\n",
    "                        \"output\": text_out,\n",
    "                    }\n",
    "                )\n",
    "            # import pdb; pdb.set_trace()\n",
    "\n",
    "        results: List[Dict] = []\n",
    "        for idx, item in enumerate(items):\n",
    "            final_text = final_texts[idx]\n",
    "            \n",
    "            if self.task in ['mbppplus', 'humanevalplus']:\n",
    "                pred = extract_markdown_python_block(final_text)\n",
    "                gold = item.get(\"gold\", \"\")\n",
    "\n",
    "                if pred is None:\n",
    "                    ok = False\n",
    "                    error_msg = \"python error: No python code block found\"\n",
    "                else:\n",
    "                    python_code_to_exe = pred + \"\\n\" + gold\n",
    "                    ok, error_msg = run_with_timeout(python_code_to_exe, timeout=10)\n",
    "    \n",
    "                print(f'=========================================')\n",
    "                print(f'Question {idx}')\n",
    "                print(f'error_msg: {error_msg}')\n",
    "\n",
    "            elif self.task in [\"aime2024\", \"aime2025\"]:\n",
    "                pred = normalize_answer(extract_gsm8k_answer(final_text))\n",
    "                gold = str(item.get(\"gold\", \"\")).strip()\n",
    "                try:\n",
    "                    pred_int = int(pred)\n",
    "                    gold_int = int(gold)\n",
    "                    ok = (pred_int == gold_int)\n",
    "                    error_msg = None\n",
    "                except ValueError:\n",
    "                    ok = False\n",
    "                    error_msg = f'Value error in parsing answer. Pred: {pred}, Gold: {gold}'\n",
    "\n",
    "            else:\n",
    "                pred = normalize_answer(extract_gsm8k_answer(final_text))\n",
    "                gold = item.get(\"gold\", \"\")\n",
    "                ok = (pred == gold) if (pred and gold) else False\n",
    "                error_msg = None\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"question\": item[\"question\"],\n",
    "                    \"gold\": gold,\n",
    "                    \"solution\": item[\"solution\"],\n",
    "                    \"context\": history_contexts[idx],\n",
    "                    \"prediction\": pred,\n",
    "                    \"raw_prediction\": final_text,\n",
    "                    \"agents\": agent_traces[idx],\n",
    "                    \"correct\": ok,\n",
    "                }\n",
    "            )\n",
    "        return results\n",
    "\n",
    "    def run_item(self, item: Dict) -> Dict:\n",
    "        return self.run_batch([item])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e6939",
   "metadata": {},
   "source": [
    "### LatentMAS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32aab8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers.cache_utils import Cache\n",
    "except ImportError:\n",
    "    Cache = None\n",
    "\n",
    "class LatentMASMethod:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: ModelWrapper,\n",
    "        *,\n",
    "        latent_steps: int = 10,\n",
    "        judger_max_new_tokens: int = 256,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.95,\n",
    "        generate_bs: int = 1,\n",
    "        args: argparse.Namespace = None,\n",
    "    ) -> None:\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        self.latent_steps = latent_steps\n",
    "        self.judger_max_new_tokens = judger_max_new_tokens\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        self.generate_bs = max(1, generate_bs)\n",
    "        self.agents = default_agents()\n",
    "        self.method_name = 'latent_mas'\n",
    "        self.vllm_device = args.device \n",
    "        self.HF_device = args.device2\n",
    "        self.latent_only = bool(getattr(args, \"latent_only\", False)) if args else False\n",
    "        self.sequential_info_only = bool(getattr(args, \"sequential_info_only\", False)) if args else False\n",
    "\n",
    "        if self.latent_only:\n",
    "            self.sequential_info_only = True\n",
    "\n",
    "        self.sampling_params = SamplingParams(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=args.max_new_tokens,\n",
    "        )\n",
    "        self.task = args.task\n",
    "\n",
    "    @staticmethod\n",
    "    def _slice_tensor(tensor: torch.Tensor, tokens_to_keep: int) -> torch.Tensor:\n",
    "        if tokens_to_keep <= 0:\n",
    "            return tensor[..., 0:0, :].contiguous()\n",
    "        keep = min(tokens_to_keep, tensor.shape[-2])\n",
    "        start = tensor.shape[-2] - keep\n",
    "        return tensor[..., start:, :].contiguous()\n",
    "\n",
    "    def _truncate_past(self, past_kv: Optional[Tuple], tokens_to_keep: int) -> Optional[Tuple]:\n",
    "        if past_kv is None or tokens_to_keep <= 0:\n",
    "            return None\n",
    "        if Cache is not None and isinstance(past_kv, Cache):\n",
    "            legacy = past_kv.to_legacy_cache()\n",
    "            trimmed_legacy = tuple(\n",
    "                tuple(self._slice_tensor(t, tokens_to_keep) for t in layer)\n",
    "                for layer in legacy\n",
    "            )\n",
    "            return past_kv.__class__.from_legacy_cache(trimmed_legacy)\n",
    "        trimmed_layers = []\n",
    "        for layer in past_kv:\n",
    "            if isinstance(layer, tuple):\n",
    "                trimmed_layers.append(tuple(self._slice_tensor(t, tokens_to_keep) for t in layer))\n",
    "            elif torch.is_tensor(layer):\n",
    "                trimmed_layers.append(self._slice_tensor(layer, tokens_to_keep))\n",
    "            else:\n",
    "                trimmed_layers.append(layer)\n",
    "        return tuple(trimmed_layers)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def run_batch(self, items: List[Dict]) -> List[Dict]:\n",
    "        if len(items) > self.generate_bs:\n",
    "            raise ValueError(\"Batch size exceeds configured generate_bs\")\n",
    "\n",
    "        batch_size = len(items)\n",
    "        past_kv: Optional[Tuple] = None\n",
    "        agent_traces: List[List[Dict]] = [[] for _ in range(batch_size)]\n",
    "        final_texts = [\"\" for _ in range(batch_size)]\n",
    "\n",
    "        for agent in self.agents:\n",
    "\n",
    "            if self.args.prompt == \"sequential\":\n",
    "                batch_messages = [\n",
    "                    build_agent_message_sequential_latent_mas(role=agent.role, question=item[\"question\"], context=\"\", method=self.method_name, args=self.args)\n",
    "                    for item in items\n",
    "                ]\n",
    "            elif self.args.prompt == \"hierarchical\":\n",
    "                batch_messages = [\n",
    "                    build_agent_message_hierarchical_latent_mas(role=agent.role, question=item[\"question\"], context=\"\", method=self.method_name, args=self.args)\n",
    "                    for item in items\n",
    "                ]\n",
    "\n",
    "\n",
    "            prompts, input_ids, attention_mask, tokens_batch = self.model.prepare_chat_batch(\n",
    "                batch_messages, add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            if agent.role != \"judger\":\n",
    "                prev_past_len = _past_length(past_kv)\n",
    "\n",
    "                if self.args.think:\n",
    "                        wrapped_prompts = [f\"{prompt}<think>\" for prompt in prompts]\n",
    "                else: \n",
    "                    wrapped_prompts = prompts\n",
    "\n",
    "                wrapped_encoded = self.model.tokenizer(\n",
    "                    wrapped_prompts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    add_special_tokens=False,\n",
    "                )\n",
    "                wrapped_ids = wrapped_encoded[\"input_ids\"].to(self.model.device)\n",
    "                wrapped_mask = wrapped_encoded[\"attention_mask\"].to(self.model.device)\n",
    "                wrapped_tokens_batch: List[List[str]] = []\n",
    "                for ids_row, mask_row in zip(wrapped_ids, wrapped_mask):\n",
    "                    active_ids = ids_row[mask_row.bool()].tolist()\n",
    "                    wrapped_tokens_batch.append(self.model.tokenizer.convert_ids_to_tokens(active_ids))\n",
    "\n",
    "                past_kv = self.model.generate_latent_batch(\n",
    "                    wrapped_ids,\n",
    "                    attention_mask=wrapped_mask,\n",
    "                    latent_steps=self.latent_steps,\n",
    "                    past_key_values=past_kv,\n",
    "                )\n",
    "                if self.sequential_info_only or self.latent_only:\n",
    "                    new_past_len = _past_length(past_kv)\n",
    "                    tokens_added = new_past_len - prev_past_len\n",
    "                    tokens_to_keep = self.latent_steps if self.latent_only else tokens_added\n",
    "                    past_kv = self._truncate_past(past_kv, tokens_to_keep)\n",
    "\n",
    "                for idx in range(batch_size):\n",
    "                    mask = wrapped_mask[idx].bool()\n",
    "                    trimmed_ids = wrapped_ids[idx][mask].to(\"cpu\").tolist()\n",
    "                    agent_traces[idx].append(\n",
    "                        {\n",
    "                            \"name\": agent.name,\n",
    "                            \"role\": agent.role,\n",
    "                            \"input\": wrapped_prompts[idx],\n",
    "                            \"input_ids\": trimmed_ids,\n",
    "                            \"input_tokens\": wrapped_tokens_batch[idx],\n",
    "                            \"latent_steps\": self.latent_steps,\n",
    "                            \"output\": \"\",\n",
    "                        }\n",
    "                    )\n",
    "            else:\n",
    "\n",
    "                past_for_decoding = past_kv if self.latent_steps > 0 else None\n",
    "\n",
    "                if self.args.think:\n",
    "                        judger_prompts = [f\"{prompt}<think>\" for prompt in prompts]\n",
    "                else: \n",
    "                    judger_prompts = prompts\n",
    "                \n",
    "                judger_encoded = self.model.tokenizer(\n",
    "                    judger_prompts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    add_special_tokens=False,\n",
    "                )\n",
    "                judger_ids = judger_encoded[\"input_ids\"].to(self.model.device)\n",
    "                judger_mask = judger_encoded[\"attention_mask\"].to(self.model.device)\n",
    "                judger_tokens_batch: List[List[str]] = []\n",
    "                for ids_row, mask_row in zip(judger_ids, judger_mask):\n",
    "                    active_ids = ids_row[mask_row.bool()].tolist()\n",
    "                    judger_tokens_batch.append(self.model.tokenizer.convert_ids_to_tokens(active_ids))\n",
    "                generated_batch, _ = self.model.generate_text_batch(\n",
    "                    judger_ids,\n",
    "                    judger_mask,\n",
    "                    max_new_tokens=self.judger_max_new_tokens,\n",
    "                    temperature=self.temperature,\n",
    "                    top_p=self.top_p,\n",
    "                    past_key_values=past_for_decoding,\n",
    "                )\n",
    "                for idx in range(batch_size):\n",
    "                    final_text = generated_batch[idx].strip()\n",
    "                    final_texts[idx] = final_text\n",
    "                    mask = judger_mask[idx].bool()\n",
    "                    trimmed_ids = judger_ids[idx][mask].to(\"cpu\").tolist()\n",
    "                    agent_traces[idx].append(\n",
    "                        {\n",
    "                            \"name\": agent.name,\n",
    "                            \"role\": agent.role,\n",
    "                            \"input\": judger_prompts[idx],\n",
    "                            \"input_ids\": trimmed_ids,\n",
    "                            \"input_tokens\": judger_tokens_batch[idx],\n",
    "                            \"output\": final_text,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        results: List[Dict] = []\n",
    "        for idx, item in enumerate(items):\n",
    "            final_text = final_texts[idx]\n",
    "            if self.task in ['mbppplus', 'humanevalplus']:\n",
    "                pred = extract_markdown_python_block(final_text)\n",
    "                gold = item.get(\"gold\", \"\")\n",
    "\n",
    "                if pred is None:\n",
    "                    ok = False\n",
    "                    error_msg = \"python error: No python code block found\"\n",
    "                else:\n",
    "                    python_code_to_exe = pred + \"\\n\" + gold\n",
    "                    ok, error_msg = run_with_timeout(python_code_to_exe, timeout=10)\n",
    "                \n",
    "                print(f'=========================================')\n",
    "                print(f'Question {idx}')\n",
    "                print(f'error_msg: {error_msg}')\n",
    "                # print(f'=========================================')\n",
    "\n",
    "            elif self.task in [\"aime2024\", \"aime2025\"]:\n",
    "                pred = normalize_answer(extract_gsm8k_answer(final_text))\n",
    "                gold = str(item.get(\"gold\", \"\")).strip()\n",
    "                try:\n",
    "                    pred_int = int(pred)\n",
    "                    gold_int = int(gold)\n",
    "                    ok = (pred_int == gold_int)\n",
    "                    error_msg = None\n",
    "                except ValueError:\n",
    "                    ok = False\n",
    "                    error_msg = f'Value error in parsing answer. Pred: {pred}, Gold: {gold}'\n",
    "\n",
    "            else:\n",
    "                pred = normalize_answer(extract_gsm8k_answer(final_text))\n",
    "                gold = item.get(\"gold\", \"\")\n",
    "                ok = (pred == gold) if (pred and gold) else False\n",
    "                error_msg = None\n",
    "            \n",
    "            results.append(\n",
    "                {\n",
    "                    \"question\": item[\"question\"],\n",
    "                    \"gold\": gold,\n",
    "                    \"solution\": item[\"solution\"],\n",
    "                    \"prediction\": pred,\n",
    "                    \"raw_prediction\": final_text,\n",
    "                    \"agents\": agent_traces[idx],\n",
    "                    \"correct\": ok,\n",
    "                }\n",
    "            )\n",
    "        return results\n",
    "    \n",
    "    def run_batch_vllm(self, items: List[Dict]) -> List[Dict]:\n",
    "        if len(items) > self.generate_bs:\n",
    "            raise ValueError(\"Batch size exceeds configured generate_bs\")\n",
    "\n",
    "        batch_size = len(items)\n",
    "        past_kv: Optional[Tuple] = None\n",
    "        agent_traces: List[List[Dict]] = [[] for _ in range(batch_size)]\n",
    "        final_texts = [\"\" for _ in range(batch_size)]\n",
    "\n",
    "        embedding_record = []\n",
    "        for agent in self.agents:\n",
    "            \n",
    "            if self.args.prompt == \"sequential\":\n",
    "                batch_messages = [\n",
    "                    build_agent_message_sequential_latent_mas(role=agent.role, question=item[\"question\"], context=\"\", method=self.method_name, args=self.args)\n",
    "                    for item in items\n",
    "                ]\n",
    "            elif self.args.prompt == \"hierarchical\":\n",
    "                batch_messages = [\n",
    "                    build_agent_message_hierarchical_latent_mas(role=agent.role, question=item[\"question\"], context=\"\", method=self.method_name, args=self.args)\n",
    "                    for item in items\n",
    "                ]\n",
    "                \n",
    "            prompts, input_ids, attention_mask, tokens_batch = self.model.prepare_chat_batch(\n",
    "                batch_messages, add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            if agent.role != \"judger\":\n",
    "                prev_past_len = _past_length(past_kv)\n",
    "\n",
    "                # to wrap all latent thoughts from previous agents\n",
    "                if self.args.think:\n",
    "                        wrapped_prompts = [f\"{prompt}<think>\" for prompt in prompts]\n",
    "                else: \n",
    "                    wrapped_prompts = prompts\n",
    "\n",
    "                wrapped_encoded = self.model.tokenizer(\n",
    "                    wrapped_prompts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    add_special_tokens=False,\n",
    "                )\n",
    "                wrapped_ids = wrapped_encoded[\"input_ids\"].to(self.model.HF_device)\n",
    "                wrapped_mask = wrapped_encoded[\"attention_mask\"].to(self.model.HF_device)\n",
    "                wrapped_tokens_batch: List[List[str]] = []\n",
    "                for ids_row, mask_row in zip(wrapped_ids, wrapped_mask):\n",
    "                    active_ids = ids_row[mask_row.bool()].tolist()\n",
    "                    wrapped_tokens_batch.append(self.model.tokenizer.convert_ids_to_tokens(active_ids))\n",
    "\n",
    "                past_kv, previous_hidden_embedding = self.model.generate_latent_batch_hidden_state(\n",
    "                    wrapped_ids,\n",
    "                    attention_mask=wrapped_mask,\n",
    "                    latent_steps=self.latent_steps,\n",
    "                    past_key_values=past_kv,\n",
    "                )\n",
    "                if self.sequential_info_only or self.latent_only:\n",
    "                    new_past_len = _past_length(past_kv)\n",
    "                    tokens_added = new_past_len - prev_past_len\n",
    "                    tokens_to_keep = self.latent_steps if self.latent_only else tokens_added\n",
    "                    past_kv = self._truncate_past(past_kv, tokens_to_keep)\n",
    "\n",
    "                if self.latent_only:\n",
    "                    if self.latent_steps > 0:\n",
    "                        previous_hidden_embedding = previous_hidden_embedding[:, -self.latent_steps:, :]\n",
    "                    else:\n",
    "                        previous_hidden_embedding = previous_hidden_embedding[:, 0:0, :]\n",
    "\n",
    "                embedding_record.append(previous_hidden_embedding)\n",
    "\n",
    "                if self.sequential_info_only or self.latent_only:\n",
    "                    embedding_record = embedding_record[-1:]\n",
    "                \n",
    "                for idx in range(batch_size):\n",
    "                    mask = wrapped_mask[idx].bool()\n",
    "                    trimmed_ids = wrapped_ids[idx][mask].to(\"cpu\").tolist()\n",
    "                    agent_traces[idx].append(\n",
    "                        {\n",
    "                            \"name\": agent.name,\n",
    "                            \"role\": agent.role,\n",
    "                            \"input\": wrapped_prompts[idx],\n",
    "                            \"input_ids\": trimmed_ids,\n",
    "                            \"input_tokens\": wrapped_tokens_batch[idx],\n",
    "                            \"latent_steps\": self.latent_steps,\n",
    "                            \"output\": \"\",\n",
    "                        }\n",
    "                    )\n",
    "            else:\n",
    "                \n",
    "                # A stack of [B, L_i, H]\n",
    "                past_embedding = torch.cat(embedding_record, dim=1).to(self.vllm_device)\n",
    "                \n",
    "                if self.args.think:\n",
    "                    judger_prompts = [f\"{prompt}<think>\" for prompt in prompts]\n",
    "                else: \n",
    "                    judger_prompts = prompts\n",
    "                \n",
    "                judger_encoded = self.model.tokenizer(\n",
    "                    judger_prompts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    add_special_tokens=False,\n",
    "                ) \n",
    "                judger_encoded = judger_encoded[\"input_ids\"].to(self.model.HF_device)\n",
    "                # Get current prompt embedding\n",
    "                curr_prompt_emb = self.model.embedding_layer(judger_encoded).squeeze(0).to(self.vllm_device)\n",
    "                \n",
    "                # assert Qwen model\n",
    "                assert \"Qwen\" in self.args.model_name or \"qwen\" in self.args.model_name, \"latent_embedding_position is only supported for Qwen models currently.\"\n",
    "\n",
    "                # handle latent embedding insertion position    \n",
    "                len_of_left = []\n",
    "                for p in judger_prompts:\n",
    "                    idx = p.find(\"<|im_start|>user\\n\")\n",
    "                    # Get the text up to and including \"<|im_start|>user\\n\"\n",
    "                    left = p[: idx + len(\"<|im_start|>user\\n\")]\n",
    "                    len_of_left.append(len(self.model.tokenizer(left)['input_ids']))\n",
    "                    \n",
    "                B, L, H = curr_prompt_emb.shape\n",
    "                _, Lp, H = past_embedding.shape  # assume shape consistency\n",
    "                    \n",
    "                whole_prompt_emb_list = []\n",
    "                for i in range(B):\n",
    "                    insert_idx = len_of_left[i]\n",
    "                    left_emb = curr_prompt_emb[i, :insert_idx, :]\n",
    "                    right_emb = curr_prompt_emb[i, insert_idx:, :]\n",
    "                    combined = torch.cat([left_emb, past_embedding[i], right_emb], dim=0)\n",
    "                    whole_prompt_emb_list.append(combined)\n",
    "\n",
    "                # Pad back to max length if needed\n",
    "                max_len = max(x.shape[0] for x in whole_prompt_emb_list)\n",
    "                whole_prompt_emb = torch.stack([\n",
    "                    torch.cat([x, torch.zeros(max_len - x.shape[0], H, device=x.device)], dim=0)\n",
    "                    for x in whole_prompt_emb_list\n",
    "                ])\n",
    "\n",
    "                # else:\n",
    "                    # Get full prompt embedding from cat with previous ones \n",
    "                    # B L H B L H\n",
    "                    # whole_prompt_emb = torch.cat([past_embedding, curr_prompt_emb], dim=1)\n",
    "                \n",
    "                # pdb.set_trace()              \n",
    "                \n",
    "                # Use vLLM \n",
    "                prompt_embeds_list = [\n",
    "                    {\n",
    "                        \"prompt_embeds\": embeds\n",
    "                    } for embeds in whole_prompt_emb \n",
    "                ]\n",
    "                \n",
    "                \n",
    "                outputs = self.model.vllm_engine.generate(\n",
    "                    prompt_embeds_list,\n",
    "                    self.sampling_params,\n",
    "                )\n",
    "\n",
    "                generated_texts = [out.outputs[0].text.strip() for out in outputs]\n",
    "                    \n",
    "                for idx in range(batch_size):\n",
    "                    text_out = generated_texts[idx].strip()\n",
    "                    final_texts[idx] = text_out\n",
    "                    agent_traces[idx].append(\n",
    "                        {\n",
    "                            \"name\": agent.name,\n",
    "                            \"role\": agent.role,\n",
    "                            \"input\": judger_prompts[idx],\n",
    "                            \"output\": text_out,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "\n",
    "        results: List[Dict] = []\n",
    "        for idx, item in enumerate(items):\n",
    "            final_text = final_texts[idx]\n",
    "            pred = normalize_answer(extract_gsm8k_answer(final_text))\n",
    "            gold = item[\"gold\"]\n",
    "            ok = (pred == gold) if (pred and gold) else False\n",
    "            results.append(\n",
    "                {\n",
    "                    \"question\": item[\"question\"],\n",
    "                    \"gold\": gold,\n",
    "                    \"solution\": item[\"solution\"],\n",
    "                    \"prediction\": pred,\n",
    "                    \"raw_prediction\": final_text,\n",
    "                    \"agents\": agent_traces[idx],\n",
    "                    \"correct\": ok,\n",
    "                }\n",
    "            )\n",
    "        return results\n",
    "\n",
    "    def run_item(self, item: Dict) -> Dict:\n",
    "        return self.run_batch([item])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5418f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(preds: List[Dict]) -> Tuple[float, int]:\n",
    "    total = len(preds)\n",
    "    correct = sum(1 for p in preds if p.get(\"correct\", False))\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    return acc, correct\n",
    "\n",
    "\n",
    "def process_batch(\n",
    "    method,\n",
    "    batch: List[Dict],\n",
    "    processed: int,\n",
    "    preds: List[Dict],\n",
    "    progress,\n",
    "    max_samples: int,\n",
    "    args: argparse.Namespace,\n",
    ") -> Tuple[int, List[Dict]]:\n",
    "    remaining = max_samples - processed\n",
    "    if remaining <= 0:\n",
    "        return processed, preds\n",
    "    current_batch = batch[:remaining]\n",
    "    if args.method == \"latent_mas\" and args.use_vllm: \n",
    "        results = method.run_batch_vllm(current_batch) \n",
    "    else:\n",
    "        results = method.run_batch(current_batch)\n",
    "    if len(results) > remaining:\n",
    "        results = results[:remaining]\n",
    "    batch_start = processed\n",
    "    for offset, res in enumerate(results):\n",
    "        preds.append(res)\n",
    "        problem_idx = batch_start + offset + 1\n",
    "        print(f\"\\n==================== Problem #{problem_idx} ====================\")\n",
    "        print(\"Question:\")\n",
    "        print(res.get(\"question\", \"\").strip())\n",
    "        agents = res.get(\"agents\", [])\n",
    "        for a in agents:\n",
    "            name = a.get(\"name\", \"Agent\")\n",
    "            role = a.get(\"role\", \"\")\n",
    "            agent_header = f\"----- Agent: {name} ({role}) -----\"\n",
    "            print(agent_header)\n",
    "            agent_input = a.get(\"input\", \"\").rstrip()\n",
    "            agent_output = a.get(\"output\", \"\").rstrip()\n",
    "            latent_steps = a.get(\"latent_steps\", None)\n",
    "            print(\"[To Tokenize]\")\n",
    "            print(agent_input)\n",
    "            if latent_steps is not None:\n",
    "                print(\"[Latent Steps]\")\n",
    "                print(latent_steps)\n",
    "            print(\"[Output]\")\n",
    "            print(agent_output)\n",
    "            print(\"----------------------------------------------\")\n",
    "        print(f\"Result: Pred={res.get('prediction')} | Gold={res.get('gold')} | OK={res.get('correct')}\")\n",
    "\n",
    "    processed += len(results)\n",
    "    if progress is not None:\n",
    "        progress.update(len(results))\n",
    "    return processed, preds\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # core args for experiments\n",
    "    parser.add_argument(\"--method\", choices=[\"baseline\", \"text_mas\", \"latent_mas\"], required=True)\n",
    "    parser.add_argument(\"--model_name\", type=str, required=True, choices=[\"Qwen/Qwen3-4B\", \"Qwen/Qwen3-4B\", \"Qwen/Qwen3-14B\"])\n",
    "    parser.add_argument(\"--max_samples\", type=int, default=100)\n",
    "    parser.add_argument(\"--task\", choices=[\"gsm8k\", \"aime2024\", \"aime2025\", \"gpqa\", \"arc_easy\", \"arc_challenge\", \"mbppplus\", 'humanevalplus', 'medqa'], default=\"gsm8k\")\n",
    "    parser.add_argument(\"--prompt\", type=str, choices=[\"sequential\", \"hierarchical\"], default=\"sequential\")\n",
    "\n",
    "    # other args\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda\")\n",
    "    parser.add_argument(\"--split\", type=str, default=\"test\")\n",
    "    parser.add_argument(\"--max_new_tokens\", type=int, default=4096)\n",
    "    parser.add_argument(\"--latent_steps\", type=int, default=10)\n",
    "    parser.add_argument(\"--temperature\", type=float, default=0.6)\n",
    "    parser.add_argument(\"--top_p\", type=float, default=0.95)\n",
    "    parser.add_argument(\"--generate_bs\", type=int, default=20)\n",
    "    parser.add_argument(\"--text_mas_context_length\", type=int, default=-1, help=\"TextMAS context length limit\")\n",
    "    parser.add_argument(\"--think\", action=\"store_true\", help=\"Manually add think token in the prompt for LatentMAS\")\n",
    "    parser.add_argument(\"--latent_space_realign\", action=\"store_true\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "\n",
    "    # for vllm support\n",
    "    parser.add_argument(\"--use_vllm\", action=\"store_true\", help=\"Use vLLM backend for generation\")\n",
    "    parser.add_argument(\"--enable_prefix_caching\", action=\"store_true\", help=\"Enable prefix caching in vLLM for latent_mas\")\n",
    "    parser.add_argument(\"--use_second_HF_model\", action=\"store_true\", help=\"Use a second HF model for latent generation in latent_mas\")\n",
    "    parser.add_argument(\"--device2\", type=str, default=\"cuda:1\")\n",
    "    parser.add_argument(\"--tensor_parallel_size\", type=int, default=1, help=\"How many GPUs vLLM should shard the model across\")\n",
    "    parser.add_argument(\"--gpu_memory_utilization\", type=float, default=0.9, help=\"Target GPU memory utilization for vLLM\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.method == \"latent_mas\" and args.use_vllm:\n",
    "        args.use_second_HF_model = True \n",
    "        args.enable_prefix_caching = True\n",
    "    \n",
    "    set_seed(args.seed)\n",
    "    device = auto_device(args.device)\n",
    "    model = ModelWrapper(args.model_name, device, use_vllm=args.use_vllm, args=args)\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    common_kwargs = dict(\n",
    "        temperature=args.temperature,\n",
    "        top_p=args.top_p,\n",
    "    )\n",
    "    if args.method == \"baseline\":\n",
    "        method = BaselineMethod(\n",
    "            model,\n",
    "            max_new_tokens=args.max_new_tokens,\n",
    "            **common_kwargs,\n",
    "            generate_bs=args.generate_bs,\n",
    "            use_vllm=args.use_vllm,\n",
    "            args=args\n",
    "        )\n",
    "    elif args.method == \"text_mas\":\n",
    "        method = TextMASMethod(\n",
    "            model,\n",
    "            max_new_tokens_each=args.max_new_tokens,\n",
    "            **common_kwargs,\n",
    "            generate_bs=args.generate_bs,\n",
    "            args=args,\n",
    "        )\n",
    "    elif args.method == 'latent_mas':\n",
    "        method = LatentMASMethod(\n",
    "            model,\n",
    "            latent_steps=args.latent_steps,\n",
    "            judger_max_new_tokens=args.max_new_tokens,\n",
    "            **common_kwargs,\n",
    "            generate_bs=args.generate_bs, \n",
    "            args=args,\n",
    "        )\n",
    "\n",
    "    preds: List[Dict] = []\n",
    "    processed = 0\n",
    "    batch: List[Dict] = []\n",
    "    \n",
    "\n",
    "    if args.task == \"gsm8k\":\n",
    "        dataset_iter = load_gsm8k(split=args.split)\n",
    "    elif args.task == \"aime2024\":\n",
    "        dataset_iter = load_aime2024(split=\"train\")\n",
    "    elif args.task == \"aime2025\":\n",
    "        dataset_iter = load_aime2025(split='train')\n",
    "    elif args.task == \"gpqa\":\n",
    "        dataset_iter = load_gpqa_diamond(split='test')\n",
    "    elif args.task == \"arc_easy\":\n",
    "        dataset_iter = load_arc_easy(split='test')\n",
    "    elif args.task == \"arc_challenge\":\n",
    "        dataset_iter = load_arc_challenge(split='test')\n",
    "    elif args.task == \"mbppplus\":\n",
    "        dataset_iter = load_mbppplus(split='test')\n",
    "    elif args.task == \"humanevalplus\":\n",
    "        dataset_iter = load_humanevalplus(split='test')\n",
    "    elif args.task == \"medqa\":\n",
    "        dataset_iter = load_medqa(split='test')\n",
    "    else:\n",
    "        raise ValueError(f'no {args.task} support')\n",
    "    \n",
    "    if args.max_samples == -1:\n",
    "        dataset_iter = list(dataset_iter)  \n",
    "        args.max_samples = len(dataset_iter)\n",
    "\n",
    "    progress = tqdm(total=args.max_samples)\n",
    "\n",
    "    for item in dataset_iter:\n",
    "        if processed >= args.max_samples:\n",
    "            break\n",
    "        batch.append(item)\n",
    "        if len(batch) == args.generate_bs or processed + len(batch) == args.max_samples:\n",
    "            processed, preds = process_batch(\n",
    "                method,\n",
    "                batch,\n",
    "                processed,\n",
    "                preds,\n",
    "                progress,\n",
    "                args.max_samples,\n",
    "                args,\n",
    "            )\n",
    "            batch = []\n",
    "            if processed >= args.max_samples:\n",
    "                break\n",
    "\n",
    "    if batch and processed < args.max_samples:\n",
    "        processed, preds = process_batch(\n",
    "            method,\n",
    "            batch,\n",
    "            processed,\n",
    "            preds,\n",
    "            progress,\n",
    "            max_samples=args.max_samples,\n",
    "            args=args,\n",
    "        )\n",
    "    progress.close()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    acc, correct = evaluate(preds)\n",
    "    print(\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"method\": args.method,\n",
    "                \"model\": args.model_name,\n",
    "                \"split\": args.split,\n",
    "                \"seed\": args.seed,\n",
    "                \"max_samples\": args.max_samples,\n",
    "                \"accuracy\": acc,\n",
    "                \"correct\": correct,\n",
    "                \"total_time_sec\": round(total_time,4),\n",
    "                \"time_per_sample_sec\": round(total_time / args.max_samples, 4),\n",
    "            },\n",
    "            ensure_ascii=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
